# Fluentd/fluentbit

## Fluentd

### Config file

It defines the entire life of an event:

```config
<source>
  @type http # input plugin
  port 8888
  bind 0.0.0.0
</source>

# capture events tagged with test.cycle
<filter test.cycle>
  @type grep
  <exclude>
    key action
    pattern ^logout$
  </exclude>
</filter>

<match test.cycle>
  @type stdout # output plugin
</match>
```

Source captures events and send them to the Fluentd routing engine. Events will go through defined `filter` directives and finally ends up in match directive. Order matters here. Top-to-bottom. Input -> filter 1 -> ... -> filter N -> Output.

#### Event
An event consists of 3 entities - tag, time, record.

The tag is a string separated by dots (e.g. myapp.access), and is used as the directions for Fluentd internal routing engine. The time field is specified by input plugins, and it must be in the Unix time format. The record is a JSON object.

```
# generated by http://<ip>:9880/myapp.access?json={"event":"data"}
tag: myapp.access
time: (current time)
record: {"event":"data"}
```

#### Use labels to break top-to-bottom processing rule

```config
<source>
  @type http
  bind 0.0.0.0
  port 8888
  @label @STAGING
</source>

# will skip
<filter test.cycle>
  @type grep
  <exclude>
    key action
    pattern ^login$
  </exclude>
</filter>

<label @STAGING>
  <filter test.cycle>
    @type grep
    <exclude>
      key action
      pattern ^logout$
    </exclude>
  </filter>

  <match test.cycle>
    @type stdout
  </match>
</label>
```

This instructs routing engine to process events on label section and hence skips non-labelled filter section.

#### Tag matching

`**` matches zero or more tag parts - a.** matches a, a.b and a.b.c.

Match directive is evaluated in order:

```
<match myapp.access>
  @type file
  path /var/log/fluent/access
</match>

# Capture all unmatched tags
<match **>
  @type blackhole_plugin
</match>
```

#### Buffer mode

Output plugins such as `forward` and `copy` supports three modes

- Non-Buffered. i.e stdout
- Synchronous Buffered
- Asynchronous Buffered

Output plugin in buffered mode first stores the received events into buffers and then writes out buffers to a destination after meeting flush conditions. So, using the buffered output, you do not see the received events immediately unlike stdout non-buffered output.

fluentd will choose the appropriate mode if `<buffer>` section is missing. A buffer is a set of chunks where a collection of events are saved. Buffers can be
stored either in memory (default) or file. In the case that the backend is unreachable (network failure or application log rejection) Fluentd automatically goes into a retry process. This could incur big memory consumption issue where undelivered logs are continuously piled up in buffer/memory awaiting retry. In this case, opt to either cap the buffer size or save buffer in files.

#### fluent-cat

```shell
# pipe message tagged log-adder.spot to fluent-cat binery as input
echo '{"app": "fluentbit-spot"}' | docker exec -i <CONTAINER> fluent-cat log-adder.spot
```

---

## FluentBit

### Config

#### Logstash_Prefix_Key

If record has the specified `key`, then the corresponding value will be used to generate index.

```
[OUTPUT]
    Name            es
    Match           *
    Host            ${ES_HOST}
    Port            443
    Logstash_Format On
    Logstash_Prefix fluentbit
    Logstash_Prefix_Key app
    Time_Key        @timestamp

<filter logs>
    @type record_modifier
    <record>
        app "fluentbit-logger"
    </record>
</filter>

// index
fluentbit-logger-2021.04.01

// index without app
fluentbit-2021.04.01
```
